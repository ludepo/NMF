{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Import texts and preprocess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel('ecb_pressconferences.xlsx', header = 0, na_values = 'NaN')\n",
    "data2 = pd.read_excel('ecb_interviews.xlsx', header=0, na_values = 'NaN')\n",
    "#data = pd.merge_ordered(data1, data2)\n",
    "#data = data.sort_values('date', ascending=False)\n",
    "type(data.text[0])\n",
    "# data.text[0].count(' ')\n",
    "data.text[0]\n",
    "\n",
    "\n",
    "# Example text, latest pressconference in 2017 before preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = data.text.tolist()\n",
    "\n",
    "nr = range(217)\n",
    "bbb =[]\n",
    "for x in nr:\n",
    "    ccc = aaa[x].count(\" \")\n",
    "    \n",
    "    bbb.append(ccc)\n",
    "    \n",
    "# create DataFrame\n",
    "datum = data.date\n",
    "ddd = pd.DataFrame(bbb, index = datum)\n",
    "\n",
    "ddd.plot(kind='line', figsize=(20,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "# preprocessor tokenizes texts first, then strips punctiation, numbers and stopwords (meaningless words) and converts\n",
    "# entire text to lowercase, Part-of-Speech-tags words to lemmatize subsequently, stem tokens and finally filter for stopwwords\n",
    "# again. Tokens are joint as a sting and returned.\n",
    "\n",
    "\n",
    "def preprocessor(clean):\n",
    "    \n",
    "    tokens = word_tokenize(clean.lower()) # tokenize text and transform to lowercase\n",
    "  \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens] # stripped = list\n",
    "    words = [word for word in stripped if word.isalpha()] # words = list \n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     finterms = pd.read_excel('Finterms_wordlist.xlsx') # include finterm dictionary by Loughran & McDonald (2011)\n",
    "#     for word in set(finterms.get('lm wordlist')):\n",
    "#         stop_words.add(word)\n",
    "    stop_words = [w.lower() for w in stop_words]\n",
    "\n",
    "    words_cleaned = [w for w in words if not w in stop_words] # tokens in lowercase that are stripped for punctuation\n",
    "\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None # for easy if-statement \n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tagged = nltk.pos_tag(words_cleaned)\n",
    "    lemmatized = []\n",
    "    \n",
    "    for word, tag in tagged:\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        if wntag is None:# not supply tag in case of None\n",
    "            lemma = lemmatizer.lemmatize(word) \n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wntag)\n",
    "        lemmatized.append(lemma)\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = []\n",
    "    for w in lemmatized:\n",
    "        stemmed.append(ps.stem(w))\n",
    "\n",
    "    words_preprocessed = [w for w in stemmed if not w in stop_words]\n",
    "     \n",
    "    text_preprocessed = \" \".join(words_preprocessed)\n",
    "   \n",
    "    return text_preprocessed\n",
    "\n",
    "\n",
    "def dataframe_preprocessor(frame):\n",
    "   \n",
    "    for cell in ['text']:\n",
    "        frame[cell] = frame[cell].apply(preprocessor)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "dataframe_preprocessed = dataframe_preprocessor(data)\n",
    "dataframe_preprocessed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_preprocessednew = dataframe_preprocessed\n",
    "dataframe_preprocessednew['nr of communications']= 1\n",
    "\n",
    "df_preprnew = dataframe_preprocessednew.drop(columns=['headline','text','type of publication'])\n",
    "df_preprnew['date']=pd.to_datetime(df_preprnew['date'])\n",
    "df_preprnew=df_preprnew.set_index('date')\n",
    "\n",
    "df_preprnew = df_preprnew.resample('Q').sum().sort_index(ascending = False)\n",
    "\n",
    "\n",
    "df_preprnew.plot(kind='bar', yticks=[2,3,4,5,6,7],figsize=(20,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Build corpus and create term-frequency-inverse-document-frequency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus = dataframe_preprocessed.text.tolist()\n",
    "print(type(corpus)) \n",
    "print(type(corpus[0]))\n",
    "print(len(corpus)) \n",
    "print(len(corpus[0]))\n",
    "print(corpus[216].count(\" \"))\n",
    "corpus[0]\n",
    "\n",
    "\n",
    "# Output shows again the latest pressconference in 2017 but this time preprocessed.\n",
    "\n",
    "# Explain corpus, picture it eg. how many words exist across corpus, most frequent words, distribution (what words in which text?\n",
    "# how did central bank press conferences develop over time?\n",
    "# -> number of publications per quarter, word lenght, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = range(217)\n",
    "acac=[]\n",
    "for x in nr:\n",
    "    ababab = corpus[x].count(\" \")\n",
    "    \n",
    "    acac.append(ababab)\n",
    "    \n",
    "# create DataFrame\n",
    "datum = dataframe_preprocessed.date\n",
    "abab = pd.DataFrame(acac, index = datum)\n",
    "\n",
    "abab.plot(kind = 'line', figsize=(20,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# development stopwords\n",
    "\n",
    "nr_stop = ddd.sub(abab)\n",
    "nr_stopperc = nr_stop.divide(ddd)\n",
    "nr_stopperc.plot(kind = 'line', figsize=(20,10))\n",
    "\n",
    "nr_stopperc.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus) # tfidf = sparse csr-matrix\n",
    "print(tfidf)\n",
    "\n",
    "\n",
    "\n",
    "# tfidf matrix indicates importance of every term for document and corpus in order to distinguish documents across corpus \n",
    "# and themes across documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# get 25 most important terms of corpus:\n",
    "feature_array = np.array(words)\n",
    "tfidf_sorting = np.argsort(tfidf.toarray()).flatten()[::-1]\n",
    "top_25 = feature_array[tfidf_sorting][:25]\n",
    "print(\"Most important 25 terms: {}\".format(top_25))\n",
    "\n",
    "# get 25 most frequent words\n",
    "vectorizer = CountVectorizer()\n",
    "tf = vectorizer.fit_transform(corpus)\n",
    "words1 = vectorizer.get_feature_names()\n",
    "feature_array1 = np.array(words1)\n",
    "tf_sorting = np.argsort(tf.toarray()).flatten()[::-1]\n",
    "frequent_25 = feature_array1[tf_sorting][:25]\n",
    "print(\"Most frequent 25 terms: {}\".format(frequent_25))\n",
    "\n",
    "\n",
    "# Below the 25 most important words indicated by the tfidf matrix are shown as well as the 25 most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Derive topics from corpus via NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Non-negative Matrix Factorization (NMF) allows to classify texts and leads to similar outputs as probablistic Latent\n",
    "# Semantic Analysis. Comparing outputs containing different quanities of topics derived led to best results of regression \n",
    "# if _____ (currently 15)  topics are to be derived\n",
    "\n",
    "\n",
    "model = NMF(n_components=15)\n",
    "\n",
    "W = nmf = model.fit_transform(tfidf)        # W = Tocpics to document\n",
    "H = model.components_                       # H = Terms to topic, factorization matrix\n",
    "                       \n",
    "    \n",
    "date = dataframe_preprocessed.date\n",
    "index = pd.DatetimeIndex(date)\n",
    "\n",
    "\n",
    "idx_to_word = np.array(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "for i, topic in enumerate(H):\n",
    " \n",
    "    print(\"Topic {}: {}\".format(i, \", \".join([str(x) for x in idx_to_word[topic.argsort()[-15:]]])))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Currently the attribution for each row in W is not a percentage, but we want to assign each document to any topic which it can be at least 10% attributed to\n",
    "sums = np.sum(W, axis=1)\n",
    "W_percent = W / sums[:, None]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(W_percent, index)\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Output describes most important words for determined topic. Below the W matrix is shown. W represents the percentage,\n",
    "# to what extend a text consists of a certian topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If more than _____ (again comparing results to derive optimal value, currently 10% 'topic limit') percent of the text was\n",
    "# devoted to a derived topic, it counted as mentioned. The code below counts the number of mentionings of the subsequent \n",
    "# topic per quarter.\n",
    "\n",
    "\n",
    "\n",
    "#How many latent topics should we look for?\n",
    "#Rundown of the challenges associated with trying to determine the number of topics to factorize into. Include nmf_similarity plot and give a rundown of the thinking behind it as well as the PCA scree plot that gives another approach to the same question.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def topic_selector(value):\n",
    "    if value < 0.25:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "        \n",
    "def dataframe_topics(data, i):\n",
    "    for cell in [i]:\n",
    "        data[cell] = data[cell].apply(topic_selector)\n",
    "  \n",
    "    return data\n",
    "\n",
    "\n",
    "ii = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "for i in ii:\n",
    "    dfw = dataframe_topics(df,i)\n",
    "# print(dfw.loc[:,dfw.any()]) #check if one topic has 0 mentions higher than 10% of text -> no, if columns are same amount\n",
    "\n",
    "dfw = dfw.resample('Q',).sum().sort_index(ascending = False) # summarize mentions during quarter \n",
    "dfw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfw.plot(kind='line', yticks=[0,1,2,3,4], colormap='gist_rainbow',figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfw.sum())\n",
    "dfw.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export matrix to excel file\n",
    "\n",
    "writer = pd.ExcelWriter('AAAAA.xlsx')\n",
    "dfw.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for visulaization\n",
    "\n",
    "# H and W\n",
    "sums = np.sum(H, axis=1)\n",
    "H_percent = H / sums[:, None]\n",
    "\n",
    "print(type(H_percent))\n",
    "print(H_percent.shape)\n",
    "\n",
    "print(type(W_percent))\n",
    "print(W_percent.shape)\n",
    "\n",
    "length = np.asarray(acac)\n",
    "print(type(length))\n",
    "print(length.shape)\n",
    "\n",
    "print(type(feature_array1))\n",
    "print(feature_array1.shape)\n",
    "\n",
    "TF = tf.toarray().transpose()\n",
    "TFF = TF.sum(axis=1)\n",
    "print(type(TFF))\n",
    "print(TFF.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "\n",
    "graph = pyLDAvis.prepare(H_percent,W_percent, length, feature_array1, TFF, R=15)\n",
    "pyLDAvis.display(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearly summarized number of mentions quarterly, topics with no metions deleted\n",
    "dfw_annually = pd.read_excel('dfw_P_15_0-25_agg-current.xlsx', sheet_name='an', index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Bulid DataFrames for dependent and independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) bulid DataFrame for independent variables\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "files = ['10_years_int_rate.xlsx','ea_debt.xlsx','euribor_int_rates.xlsx','exchange_rates.xlsx','gdp_growth.xlsx','m3_growth.xlsm','unemployment_rate.xlsx', 'time.xlsx']\n",
    "\n",
    "dataframes = [pd.ExcelFile(f) for f in files]\n",
    "\n",
    "intr = dataframes[0].parse('annually_aggregated')\n",
    "debt = dataframes[1].parse('annually_aggregated')\n",
    "euribor = dataframes[2].parse('annually_aggregated')\n",
    "exch = dataframes[3].parse('annually_aggregated')\n",
    "gdp = dataframes[4].parse('annually_aggregated')\n",
    "m3 = dataframes[5].parse('annually_aggregated')\n",
    "unempl = dataframes[6].parse('annually_aggregated')\n",
    "time = dataframes[7].parse('Tabelle1')\n",
    "\n",
    "frames = [intr] \n",
    "frames.append(euribor.delta_euriday_an)\n",
    "frames.append(exch.delta_dollar_an) \n",
    "frames.append(gdp.delta_gdp_an) \n",
    "frames.append(m3.delta_m3_an) \n",
    "frames.append(time.fincri)\n",
    "frames.append(time.year) \n",
    "\n",
    "# frames.append(debt.delta_debt_an)\n",
    "# frames.append(euribor.delta_eurimon_an)\n",
    "# frames.append(euribor.delta_euri3mon_an) \n",
    "# frames.append(euribor.delta_euri6mon_an)\n",
    "# frames.append(euribor.delta_euri12mon_an) \n",
    "# frames.append(exch.delta_pound_an) \n",
    "# frames.append(exch.delta_franc_an) \n",
    "# frames.append(exch.delta_yen_an) \n",
    "# frames.append(unempl.delta_unempl_an)\n",
    "# frames.append(time.qter) \n",
    "\n",
    "\n",
    "\n",
    "new_df = pd.concat(frames, axis=1)\n",
    "\n",
    "\n",
    "ndf = new_df.set_index('quarter')\n",
    "ndf = ndf.resample('Q').sum().sort_index(ascending = False)\n",
    "\n",
    "\n",
    "masterframe = pd.concat([dfw_annually, ndf], axis = 1)\n",
    "masterframe\n",
    "\n",
    "\n",
    "# Prossible control variables are the quarterly data on annually aggregated changes in:\n",
    "\n",
    "#    - 10 years interest rate (StatisticalDataWarehouse: IRS.M.U2.L.L40.CI.0000.EUR.N.Z) \n",
    "#    - Government debt (Eurostat: Quarterly government debt [gov_10q_ggdebt])\n",
    "#    - euribor rates (Eurostat: Geldmarktzinssätze - Vierteljährliche Daten [irt_st_q])\n",
    "#    - exchange rates [Pund Sterling, Swiss Franc, Yen, USD] (Eurostat: Euro/Ecu-Wechselkurse - Vierteljährliche Daten [ert_bil_eur_q])\n",
    "#    - GDP (StatisticalDataWarehouse: MNA.Q.Y.I8.W2.S1.S1.B.B1GQ._Z._Z._Z.EUR.LR.GY)\n",
    "#    - Money aggregate M3 (StatisticalDataWarehouse: BSI.Q.U2.N.V.M30.X.I.U2.2300.Z01.A)\n",
    "#    - Unemployment rate (Eurostat: Unemployment by sex and age - quarterly average [une_rt_q])\n",
    "\n",
    "#    - variable for year\n",
    "#    - variable for quarter\n",
    "#    - dummy for financial crisis 07/08\n",
    "\n",
    "# due to strong correlations within the variables, most are excluded as shown in the next section.\n",
    "# The ouput contains the mentions of the derived topics along with the selected controll variables. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "masterframe_judg = masterframe.drop(masterframe.index[0:4])\n",
    "masterframe_judg = masterframe_judg.reset_index()\n",
    "masterframe_judg = masterframe_judg.drop('index', axis=1)\n",
    "masterframe_judg\n",
    "\n",
    "ndf_judg = ndf.drop(ndf.index[0:4])\n",
    "ndf_judg = ndf_judg.reset_index()\n",
    "ndf_judg = ndf_judg.drop('quarter', axis=1)\n",
    "ndf_judg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) bulid Dataframe for dependent variable\n",
    "\n",
    "\n",
    "spf_errs = pd.read_excel('SPF_errors.xlsx', sheet_name = 'data')\n",
    "spf_errs = spf_errs.set_index('quarter')\n",
    "spf_errs = spf_errs.resample('Q').sum().sort_index(ascending = False)\n",
    "spf_errs = spf_errs[:68]\n",
    "spf_errs24 = spf_errs.err_24_month[:72]\n",
    "\n",
    "\n",
    "# ABSOLUTE VALUES\n",
    "spf_errs = spf_errs.abs()\n",
    "spf_errs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DataFrame shows the quarterly inflation forecast error of SPF 12 and 24 month rolling forecast.\n",
    "# The HICP inflation is taken from the ECBs StatisticalDataWarehouse (ICP.M.U2.N.000000.3.ANR),\n",
    "# as well as the SPF 12 month rolling forecast (SPF.M.U2.HICP.POINT.P12M.Q.AVG) and the 24 month rolling forecast\n",
    "# (SPF.M.U2.HICP.POINT.P24M.Q.AVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spf_errs_judg = spf_errs.reset_index()\n",
    "spf_errs_judg = spf_errs_judg.drop('quarter', axis=1)\n",
    "spf_errs_judg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation among topics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "mask = np.zeros_like(dfw_annually.corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(data=dfw_annually.corr(),mask=mask,vmin=-1, vmax=1, square=True, cmap=\"YlGnBu\",linewidths=.5, ax=ax)\n",
    "dfw_annually.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation among control variables\n",
    "\n",
    "mask = np.zeros_like(ndf.corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "print(sns.heatmap(data=ndf.corr(), mask=mask, vmin=-1, vmax=1, square=True, cmap=\"YlGnBu\",linewidths=.5))\n",
    "ndf.corr()\n",
    "\n",
    "# correlations among control variables (>0,5):\n",
    "\n",
    "# intr, euri6.        0,50                       # euriday, eurimon:   0,99\n",
    "# intr, euri12:       0,57                       # euriday, euri3mon:  0,98\n",
    "                                                 # euriday, euri6mon:  0,96\n",
    "# debt, euriday:      0.65                       # euriday, euri12mon: 0,92\n",
    "# debt, eurimon:      0,63                       # euriday, gdp:       0,84\n",
    "# debt, euri3mon:     0,63                       # euriday, unempl:   -0,77\n",
    "# debt, euri6mon:     0,63                       # eurimon, euri3:     0,99\n",
    "# debt, euri12mon:    0,57                       # eurimon, euri6:     0,98\n",
    "# debt, gdp:         -0,71                       # eurimon, euri12:    0,95\n",
    "# debt, unempl:       0,79                       # eurimon, gdp:       0,86\n",
    "                                                 # eurimon, unempl:   -0,78\n",
    "# franc, yen:         0,56                       # eurimon3, euri6:    0,99\n",
    "# franc, dollar:      0,55                       # eurimon3, euri12:   0,97\n",
    "# yen, dollar:        0,62                       # eurimon3, gdp:      0,87\n",
    "                                                 # eurimon3, unempl:  -0,78\n",
    "# gdp, unempl:       -0,88                       # euri6, euri12:      0,99\n",
    "                                                 # euri6, gdp:         0,86\n",
    "# m3, ficri:          0,62                       # euri6, unempl:     -0,77\n",
    "                                                 # euri12, gdp:        0,84\n",
    "                                                 # euri12, unempl:    -0,75\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation between topics and controll variables\n",
    "\n",
    "mask = np.zeros_like(masterframe.corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "fig, ax = plt.subplots(figsize=(14,13))\n",
    "sns.heatmap(data=masterframe.corr(), mask=mask, vmin=-1, vmax=1, square=True, cmap=\"YlGnBu\",linewidths=.9, ax=ax)\n",
    "\n",
    "masterframe.corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 6) Run regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS 12 month rolling forecast error\n",
    "masterframe_judg = sm.add_constant(masterframe_judg)\n",
    "model = sm.OLS(spf_errs_judg.err_12_month, masterframe_judg)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
